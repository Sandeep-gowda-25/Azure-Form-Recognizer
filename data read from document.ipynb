{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This isd code to read data from Form recognizer\n",
    "## Also, this has steps to add additional metadata on pricessing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.cosmos import CosmosClient, DatabaseProxy, ContainerProxy\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blobservice_client(account_url):\n",
    "    credential = DefaultAzureCredential()\n",
    "    blobServicClient = BlobServiceClient(account_url=account_url, credential=credential)\n",
    "    return blobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Code to read file data stored in azure storage account\n",
    "\n",
    "def get_blob_contents(fileName, account_url, container):\n",
    "    \"\"\"This function is used to access the blob file and load the data into redis\n",
    "\n",
    "    Args:\n",
    "        fileName : name of the file\n",
    "        account_url : url of the storage account\n",
    "        container : name of the container\n",
    "\n",
    "    Returns:\n",
    "        file data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        blobServicClient = get_blobservice_client(account_url)\n",
    "        blobClient = blobServicClient.get_blob_client(container=container, blob=fileName)\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            file_path = f\"{temp_dir}/{container}/{fileName}\"\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            with open(f\"{file_path}\", \"wb\") as file:\n",
    "                blob_data = blobClient.download_blob()\n",
    "                blob_data.readinto(file)\n",
    "            content = open(f\"{file_path}\",'rb')\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception occured in reading blob: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = <> # blob file name to be downloaded\n",
    "container_name = <> # container having target blob\n",
    "stg_account_url = <> # storage account url having data\n",
    "filecontents = get_blob_contents(fileName = filename, account_url = stg_account_url, container=container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.formrecognizer import FormRecognizerClient,DocumentTable\n",
    "import json\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below is code to connect to azure form recognizer service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_endpoint = <> # Form Recognizer endpoint\n",
    "def get_document_analysis_client(endpoint):\n",
    "    credential = DefaultAzureCredential()\n",
    "    document_analysis_client = DocumentAnalysisClient(endpoint=endpoint, credential=credential)\n",
    "    return document_analysis_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is code to get contents analysed from form recognizers document intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_contents(filecontents):\n",
    "    document_analysis_client = get_document_analysis_client(DI_endpoint)\n",
    "    poller = document_analysis_client.begin_analyze_document(\"prebuilt-document\",document=filecontents)\n",
    "    result = poller.result()\n",
    "    document = DocumentTable(document_analysis_client)\n",
    "    document.from_pdf(filecontents)\n",
    "    return document\n",
    "all_contents = read_file_contents(filecontents)\n",
    "contents = all_contents.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOw from the document intelligence output, data will be segregated in different parts like paragraphs, tables, key value pairs,... \n",
    "### now if a page containes paragraphs at the begining and table in the middle and again paragraphs at the end, then correct position of table in page for reconstruction will be needed, \n",
    "### so below is code to fetch those details and then organize them in proper sequence based on spans and bounding region details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs(contents):\n",
    "    para_data = []\n",
    "    page_width =  result_in_dict['pages'][0]['width']\n",
    "    for i, paragraph in enumerate(paragraphs, start=1):\n",
    "        placement=''\n",
    "        if(paragraph[\"bounding_regions\"][0][\"polygon\"][0][\"x\"]>=(page_width/2)):\n",
    "            placement=\"right\"\n",
    "        else:\n",
    "            placement=\"left\"\n",
    "        paragraph_entry = {\n",
    "            \"content\": paragraph[\"content\"],\n",
    "            \"role\": paragraph.get(\"role\", \"NA\"),\n",
    "            \"page no\": paragraph[\"bounding_regions\"][0][\"page_number\"],\n",
    "            \"spans\":paragraph[\"spans\"],\n",
    "            \"placement\":placement,\n",
    "            \"y-cordniate\":paragraph[\"bounding_regions\"][0][\"polygon\"][0]['y'],\n",
    "            \"matching_ordinal\":[]\n",
    "        }\n",
    "        para_data.append(paragraph_entry)\n",
    "        return para_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = get_paragraphs(contents['paragraphs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(contents):\n",
    "    table_data=[]\n",
    "    for i, table in enumerate(tables, start=0):\n",
    "        for cell in table[\"cells\"]:\n",
    "            table_entry = {\n",
    "                \"content\": cell[\"content\"],\n",
    "                \"role\": \"table cell\",\n",
    "                \"page no\": cell[\"bounding_regions\"][0][\"page_number\"],\n",
    "                \"spans\":cell[\"spans\"],\n",
    "                \"table no\":i\n",
    "            }\n",
    "            table_data.append(table_entry)\n",
    "    return table_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = get_tables(contents[\"tables\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(para_data,table_data):\n",
    "    for table in table_data:\n",
    "        for i,para in enumerate(para_data):\n",
    "            if(para[\"page no\"]==table[\"page no\"]):\n",
    "                if(para[\"content\"]==table[\"content\"]):\n",
    "                    if(para[\"spans\"]==table[\"spans\"]):\n",
    "                        para[\"content\"]=\"\"\n",
    "                        para[\"role\"]=\"table\"\n",
    "                        para[\"table no\"]=table[\"table no\"]\n",
    "    data = pd.DataFrame(para_data)\n",
    "    if 'table no' in data.columns:\n",
    "        data['table no'] = data['table no'].fillna(np.nan).astype('Int64')\n",
    "        data = data.drop_duplicates(subset=['table no', 'content'], keep='first')\n",
    "    else:\n",
    "        print(\"The 'table no' column does not exist in the DataFrame.\")\n",
    "    final_list=data.to_dict(orient='records')\n",
    "    return final_list\n",
    "filtered_list = filter_duplicates(paragraphs,table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_extra_metadata(final_list):\n",
    "    for i,item in enumerate(final_list):\n",
    "        item['ordinal']=i\n",
    "        if(item[\"role\"]==\"table\"):\n",
    "            table_no=item[\"table no\"]\n",
    "            content_for_table = table_data[table_no]\n",
    "            for j in range(len(content_for_table['cells'])): \n",
    "                content_for_table['cells'][j].pop('bounding_regions') \n",
    "                content_for_table['cells'][j].pop('spans')\n",
    "            content_for_table.pop('bounding_regions')\n",
    "            content_for_table.pop('spans')\n",
    "            item[\"content\"]=content_for_table\n",
    "        elif(item[\"role\"]=='NA'):\n",
    "            item[\"role\"]=\"para\"\n",
    "        elif(item[\"role\"]==None):\n",
    "            item[\"role\"]=\"para\"\n",
    "filtered_list = drop_extra_metadata(filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fina; filtered list contain all paras and tables in proper sequence with no repeated entires, additionally we'll get metadatas like role table/para, page mp. contents, table no,..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
